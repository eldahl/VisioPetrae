name: Deploy Inference Server

on:
  push:
    branches: [ deploy-inference-server ]
    paths:
      - 'inference_server/**'
  pull_request:
    branches: [ deploy-inference-server ]
    paths:
      - 'inference_server/**'
  workflow_dispatch:  # Allows manual triggering

jobs:
  deploy-inference-server:
    runs-on: inference-server
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Install dependencies and download model
      shell: bash -l {0}
      env:
        PIP_NO_BUILD_ISOLATION: 1
      run: |
        set -euo pipefail
        cd inference_server
        conda init
        #source /opt/anaconda/etc/profile.d/conda.sh
        eval "$(conda shell.bash hook)"

        # Download Ovis repo
        git clone https://github.com/AIDC-AI/Ovis.git
        
        # Setup python virtual environment
        conda env remove -n ovis-env -y
        conda create -n ovis-env python=3.10 -y
        conda activate ovis-env
        
        #pip install --upgrade pip
        #pip cache purge

        # Install requirements for the project through pip
        #cd ..
        #~/.conda/envs/ovis-env/bin/pip install -r requirements.in
        #pip install -e .
        pip install torch==2.4.0
        pip install transformers==4.49.0 pillow==10.3.0
        pip install flash-attn==2.7.0.post2 --no-build-isolation
        pip install gptqmodel
        pip install numpy==1.25.0
        #pip install auto-gptq optimum
        #pip install torch==2.4.0 transformers==4.49.0 pillow==11.2.1 flash-attn==2.7.0.post2 gptqmodel fastapi[standard] numpy==1.25.0
        pip install gptqmodel --force-reinstall
        
        # Install fastapi
        pip install "fastapi[standard]"

        pip check

        # Download 8b-GPTQ-Int4 model files
        cd Ovis
        mkdir models
        cd models
        git clone https://huggingface.co/AIDC-AI/Ovis2-8B-GPTQ-Int4
        cd ..

        echo "Python path: $(which python)"
        echo "Pip path: $(which pip)"
        pip list
        
    - name: Start inference server
      shell: bash -l {0} 
      run: |
        cd inference_server
       
        eval "$(conda shell.bash hook)" 
        conda activate ovis-env
        
        echo "Python path: $(which python)"
        echo "Pip path: $(which pip)"
        pip list
        
        tmux new-session -d -s inference_server "OVIS_MODEL=Ovis/models/Ovis2-8B-GPTQ-Int4 fastapi run infer.py > inference_server.log 2>&1"
